{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7551805f",
   "metadata": {},
   "source": [
    "### 0. Installing Transformers and Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "638e13c5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp310-cp310-win_amd64.whl (3.3 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.23.0-cp310-cp310-win_amd64.whl (14.6 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.6.2-cp310-cp310-win_amd64.whl (262 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aviparna.biswas\\anaconda3\\envs\\arc\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aviparna.biswas\\anaconda3\\envs\\arc\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aviparna.biswas\\anaconda3\\envs\\arc\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\aviparna.biswas\\anaconda3\\envs\\arc\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aviparna.biswas\\anaconda3\\envs\\arc\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\aviparna.biswas\\anaconda3\\envs\\arc\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aviparna.biswas\\anaconda3\\envs\\arc\\lib\\site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aviparna.biswas\\anaconda3\\envs\\arc\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Installing collected packages: pyparsing, tqdm, packaging, filelock, tokenizers, regex, numpy, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.7.1 huggingface-hub-0.8.1 numpy-1.23.0 packaging-21.3 pyparsing-3.0.9 regex-2022.6.2 tokenizers-0.12.1 tqdm-4.64.0 transformers-4.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dcce373",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False # Enable Jupyter auto-complete function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d01a7",
   "metadata": {},
   "source": [
    "### 1. Load Question Generating Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad477d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aviparna.biswas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "713c0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueGenerator():\n",
    "  def __init__(self):\n",
    "    self.que_model = T5ForConditionalGeneration.from_pretrained('./t5_que_gen_model/t5_base_que_gen/')\n",
    "    self.ans_model = T5ForConditionalGeneration.from_pretrained('./t5_ans_gen_model/t5_base_ans_gen/')\n",
    "\n",
    "    self.que_tokenizer = T5Tokenizer.from_pretrained('./t5_que_gen_model/t5_base_tok_que_gen/')\n",
    "    self.ans_tokenizer = T5Tokenizer.from_pretrained('./t5_ans_gen_model/t5_base_tok_ans_gen/')\n",
    "    \n",
    "    self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    self.que_model = self.que_model.to(self.device)\n",
    "    self.ans_model = self.ans_model.to(self.device)\n",
    "  \n",
    "  def generate(self, text):\n",
    "    answers = self._get_answers(text)\n",
    "    questions = self._get_questions(text, answers)\n",
    "    output = [{'answer': ans, 'question': que} for ans, que in zip(answers, questions)]\n",
    "    return output\n",
    "  \n",
    "  def _get_answers(self, text):\n",
    "    # split into sentences\n",
    "    sents = sent_tokenize(text)\n",
    "\n",
    "    examples = []\n",
    "    for i in range(len(sents)):\n",
    "      input_ = \"\"\n",
    "      for j, sent in enumerate(sents):\n",
    "        if i == j:\n",
    "            sent = \"[HL] %s [HL]\" % sent\n",
    "        input_ = \"%s %s\" % (input_, sent)\n",
    "        input_ = input_.strip()\n",
    "      input_ = input_ + \" </s>\"\n",
    "      examples.append(input_)\n",
    "    \n",
    "    batch = self.ans_tokenizer.batch_encode_plus(examples, max_length=512, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "      outs = self.ans_model.generate(input_ids=batch['input_ids'].to(self.device), \n",
    "                                attention_mask=batch['attention_mask'].to(self.device), \n",
    "                                max_length=32,\n",
    "                                # do_sample=False,\n",
    "                                # num_beams = 4,\n",
    "                                )\n",
    "    dec = [self.ans_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
    "    answers = [item.split('[SEP]') for item in dec]\n",
    "    answers = chain(*answers)\n",
    "    answers = [ans.strip() for ans in answers if ans != ' ']\n",
    "    return answers\n",
    "  \n",
    "  def _get_questions(self, text, answers):\n",
    "    examples = []\n",
    "    for ans in answers:\n",
    "      input_text = \"%s [SEP] %s </s>\" % (ans, text)\n",
    "      examples.append(input_text)\n",
    "    \n",
    "    batch = self.que_tokenizer.batch_encode_plus(examples, max_length=512, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "      outs = self.que_model.generate(input_ids=batch['input_ids'].to(self.device), \n",
    "                                attention_mask=batch['attention_mask'].to(self.device), \n",
    "                                max_length=32,\n",
    "                                num_beams = 4)\n",
    "    dec = [self.que_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
    "    return dec "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3fdd9",
   "metadata": {},
   "source": [
    "### 2. Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51e2c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_generator = QueGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47678b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"For other organizations, the attendance feature is working fine. This seems to be a permission issue from the admin portal.Please check from the admin portal whether the attendance features have been given to the users or not.Please feel free to call in case of any questions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9433b9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aviparna.biswas\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\aviparna.biswas\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:219: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'answer': '<pad> other organizations',\n",
       "  'question': '<pad> For what type of organizations is the attendance feature working fine?</s> <pad> <pad> <pad> <pad> <pad>'},\n",
       " {'answer': '</s>',\n",
       "  'question': '<pad> What is the permission issue with the attendance feature?</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'},\n",
       " {'answer': '<pad> permission issue',\n",
       "  'question': '<pad> What does the admin portal think is the reason for the failure of the attendance feature?</s>'},\n",
       " {'answer': '</s>',\n",
       "  'question': '<pad> What is the permission issue with the attendance feature?</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_generator.generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e52ca6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"The PCAOB staff provides guidance to help firms when implementing CAM requirements. Staff may update this guidance as needed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5968361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aviparna.biswas\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\aviparna.biswas\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:219: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'answer': '<pad> PCAOB',\n",
       "  'question': '<pad> Who provides guidance to firms when implementing CAM requirements?</s> <pad> <pad> <pad> <pad>'},\n",
       " {'answer': '</s>',\n",
       "  'question': '<pad> What does the PCAOB provide to firms when implementing CAM requirements?</s>'},\n",
       " {'answer': '<pad> Staff',\n",
       "  'question': '<pad> Who provides guidance to firms when implementing CAM requirements?</s> <pad> <pad> <pad> <pad>'},\n",
       " {'answer': '</s> <pad> <pad>',\n",
       "  'question': '<pad> What type of guidance does the PCAOB provide?</s> <pad> <pad> <pad> <pad> <pad>'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_generator.generate(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "009d33af",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"NexAEI is a mobile-based attendance capturing system that helps you manage the decentralized attendance of your hybrid workforce. Transform the payroll experience in your organization by leveraging our advanced attendance capturing modes and easily integrate the data with your existing ERP system.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "464477b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aviparna.biswas\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "C:\\Users\\aviparna.biswas\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:219: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'answer': '<pad> NexAEI',\n",
       "  'question': '<pad> What is the name of the mobile-based attendance capturing system?</s>'},\n",
       " {'answer': '</s>',\n",
       "  'question': '<pad> What is the purpose of NexAEI?</s> <pad> <pad> <pad> <pad> <pad>'},\n",
       " {'answer': '<pad> ERP system',\n",
       "  'question': '<pad> What does NexAEI integrate with?</s> <pad> <pad> <pad> <pad> <pad> <pad>'},\n",
       " {'answer': '</s> <pad>',\n",
       "  'question': \"<pad> What is NexAEI's advanced attendance capturing modes?</s> <pad>\"}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "que_generator.generate(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935b4dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ARC",
   "language": "python",
   "name": "arc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
